{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7d16ac-b439-49d4-8a25-cb5a81686af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from load_dataset import create_grid_dataset, save_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cc086-f960-4e56-ba99-ed1e11f1b01b",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Loading raw dataset into serialized file of PyTorch geometric dataset (pickle)\n",
    "\n",
    "The raw data is used on this project is from Bentivoglio Roberto, & Bruijns Ron. (2023). Raw datasets for paper \"Rapid Spatio-Temporal Flood Modelling via Hydraulics-Based Graph Neural Networks\" [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7764418. \n",
    "\n",
    "Raw datasets contain elevation (DEM), water depth (WD) in time and velocities (in x and y direction, VX and VY) in time. The datasets contain 4 subsets, which are divided as training and validation dataset, and 3 training datasets. \n",
    "\n",
    "In this project, the model training will be done using the training and validation dataset. The proposed model will be tested out for testing dataset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6e72c7-a0a3-42b3-ba57-5b5a305cc0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prayl\\OneDrive - Delft University of Technology\\06 Q6 - DSAI\\project\\Flood-5\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [00:13<00:00,  5.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# # make serialized dataset for training and testing dataset\n",
    "# # only use the first testing dataset \n",
    "# # this part of the code can be left out, as the training dataset and 1st testing dataset is already pickled\n",
    "\n",
    "# # local folder path for dataset\n",
    "# dataset_folder = r'C:\\Users\\prayl\\OneDrive - Delft University of Technology\\06 Q6 - DSAI\\raw_datasets'\n",
    "\n",
    "# # make two dataset for \"training\" and \"testing\"\n",
    "# n_sim = [80, 20]\n",
    "# start_sim = [1, 501]\n",
    "# dataset_name = ['grid_train', 'grid_test']\n",
    "\n",
    "# # create a folder for datasets\n",
    "# datasets_folder = 'datasets' \n",
    "# if not os.path.exists(datasets_folder):\n",
    "#     os.makedirs(datasets_folder)\n",
    "    \n",
    "# dataset_dir = datasets_folder \n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # convert into pyTorch Graphic object \n",
    "# for i in range (2):\n",
    "#     pyg_dataset = create_grid_dataset(dataset_folder, n_sim=n_sim[i])\n",
    "#     save_database(pyg_dataset, name=dataset_name[i], out_path=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fcede1d-c73c-4e7b-98f4-c031740c9445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 16128], edge_distance=[16128], edge_slope=[16128], edge_relative_distance=[16128, 2], num_nodes=4096, pos=[4096, 2], DEM=[4096], WD=[4096, 97], VX=[4096, 97], VY=[4096, 97])\n"
     ]
    }
   ],
   "source": [
    "# Download the flood arrival time dataset (if necessary)\n",
    "\n",
    "train_path = r\"datasets\\grid_train.pkl\"\n",
    "test_path = r\"datasets\\grid_test.pkl\"\n",
    "\n",
    "train_url = \"https://drive.google.com/uc?export=download&id=107RJdK_geFV7kEUXeiHcunhNsWA6OXt5&confirm=t&uuid=d7cb6bcc-f8e5-4a22-884e-351a671e8e78&at=AB6BwCAwzFb0LRRuvwxOFTbLuh4m:1702988223954\"\n",
    "test_url = \"https://drive.google.com/uc?export=download&id=1l2_LBJDf1-menT51Vj2JPff3hCUBlRZu&confirm=t&uuid=9523c5e7-a2af-47e6-b32a-9a0d6f0dcbfd&at=AB6BwCC_YV4IdESbvLKU0zDWMH4s:1702989314091\"\n",
    "\n",
    "# retrieve data from url\n",
    "if not os.path.isfile(train_path):\n",
    "    print(\"Downloading the pickled dataset...\")\n",
    "    urlretrieve(train_url, r\"datasets\\grid_train.pkl\")\n",
    "    urlretrieve(test_url, r\"datasets\\grid_test.pk\")\n",
    "\n",
    "with open('datasets\\grid_train.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open('datasets\\grid_test.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "    \n",
    "# Now 'data' contains the deserialized object\n",
    "print(train_dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
