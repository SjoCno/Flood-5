{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3637e8",
   "metadata": {},
   "source": [
    "# Path of _autoregressive_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d73edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"CNN_auto_regressive models/\"\n",
    "saved_model = \"autoregmodel_v1_t48_lr_0.0005_epochs_300_batches_16\" + \".pth\"\n",
    "\n",
    "pre_trained_model = folder + saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358f928",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90661f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Additional input\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "# !pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "# !pip install perlin-noise\n",
    "from perlin_noise import PerlinNoise\n",
    "import random\n",
    "from loader import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "from cycler import cycler\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set the color scheme\n",
    "sns.set_theme()\n",
    "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15094805",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e03bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'raw_datasets/'\n",
    "train_dataset = 'DEM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7ab6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = data_folder\n",
    "n_sim = 80\n",
    "start_sim = 1\n",
    "dataset_name = 'grid'\n",
    "\n",
    "datasets_folder = 'datasets'\n",
    "if not os.path.exists(datasets_folder):\n",
    "    os.makedirs(datasets_folder)\n",
    "    \n",
    "dataset_dir = datasets_folder + '/train'\n",
    "\n",
    "\n",
    "##################### Use this code to create local pickle file #####################\n",
    "#pyg_dataset = create_grid_dataset(dataset_folder, n_sim=n_sim)\n",
    "#save_database(pyg_dataset, name=dataset_name, out_path=dataset_dir)\n",
    "\n",
    "train_dataset = load_dataset(dataset_name=dataset_name, dataset_folder=dataset_dir)\n",
    "test_dataset = load_dataset(dataset_name=dataset_name, dataset_folder=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da29e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, scaler_DEM, scaler_WD):\n",
    "    \n",
    "    min_DEM, max_DEM = scaler_DEM.data_min_[0], scaler_DEM.data_max_[0]\n",
    "    min_WD, max_WD = scaler_WD.data_min_[0], scaler_WD.data_max_[0]\n",
    "    normalized_dataset = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        DEM = dataset[idx]['DEM']\n",
    "        WD = dataset[idx]['WD']\n",
    "        norm_DEM = (DEM - min_DEM) / (max_DEM - min_DEM)\n",
    "        norm_WD = (WD - min_WD) / (max_WD - min_WD)\n",
    "\n",
    "        DEM = norm_DEM.reshape(64,64)\n",
    "        WD = norm_WD[:,0].reshape(64,64)\n",
    "        \n",
    "        temp_dict = {}\n",
    "        temp_dict['Input'] = torch.stack((DEM, WD), dim=0)\n",
    "\n",
    "        WD_transposed = norm_WD[:, 1:].reshape(64,64, -1)\n",
    "        WD_transposed = WD_transposed.transpose(0, 2)\n",
    "        WD_transposed = WD_transposed.transpose(1, 2)\n",
    "        temp_dict['WD'] = WD_transposed\n",
    "    \n",
    "        normalized_dataset.append(temp_dict)\n",
    "    \n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae88044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_DEM = MinMaxScaler() # Can store DEM, VX, VY as one 'input' Scaler\n",
    "scaler_WD = MinMaxScaler()\n",
    "\n",
    "for index in range(len(train_dataset)): # =80\n",
    "    scaler_DEM.partial_fit(train_dataset[index]['DEM'].reshape(-1, 1).cpu())\n",
    "    scaler_WD.partial_fit(train_dataset[index]['WD'].reshape(-1, 1).cpu())\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_DEM, scaler_WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33416845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and testing\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "training_dataset, val_dataset = random_split(normalized_train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de280c1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e04941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "        layers.append(nn.PReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
    "\n",
    "        self.cnnblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[32, 64, 128], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            outs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return outs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[128, 64, 32], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=2)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "             for block in range(len(channels)-1)]\n",
    "             )\n",
    "\n",
    "    def forward(self, x, x_skips):\n",
    "        for i in range(len(x_skips)):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "\n",
    "        x = self.dec_blocks[-1](x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True,\n",
    "                 bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
    "        encoder_channels = [node_features]+hidden_channels\n",
    "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
    "\n",
    "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x[-1], x[:-1])\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8211ff6",
   "metadata": {},
   "source": [
    "# Get losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0c2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(model, loader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['Input']\n",
    "            y = batch['WD']  # [:,:,:,1]\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954ac64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_loader = DataLoader(normalized_train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ec7d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model validation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m get_errors(\u001b[43mmodel\u001b[49m, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Model validation\n",
    "test_loss = get_errors(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988e69d",
   "metadata": {},
   "source": [
    "# Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_loss, bins=20, edgecolor='black')\n",
    "\n",
    "plt.title('Histogram of Given Values')\n",
    "plt.xlabel('Errors')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.xlim(0.000, 0.250)\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2d340",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one sample\n",
    "data_id = 70\n",
    "x = normalized_train_dataset[data_id]['Input'].reshape(1, 2, 64, 64)\n",
    "\n",
    "pred_WD = model(x).detach()\n",
    "WD = normalized_train_dataset[data_id]['WD'][-1,:,:]\n",
    "pred_last_WD = pred_WD[:,-1,:,:]\n",
    "\n",
    "\n",
    "show_WD = WD.reshape(64,64)\n",
    "show_pred_WD = pred_last_WD.reshape(64,64)\n",
    "\n",
    "number_grids = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(show_WD.reshape(number_grids,number_grids), cmap='Blues_r', origin='lower')\n",
    "axs[0].set_title('WD target')\n",
    "\n",
    "axs[1].imshow(show_pred_WD.reshape(number_grids,number_grids), cmap='Blues_r', origin='lower')\n",
    "axs[1].set_title('WD prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff3112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
