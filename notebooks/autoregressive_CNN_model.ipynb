{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4SsZbVLFE9xe",
    "outputId": "a8471faa-1b71-412d-f41b-9cd19e72d256"
   },
   "outputs": [],
   "source": [
    "## Useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Additional input\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "# !pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "# !pip install perlin-noise\n",
    "from perlin_noise import PerlinNoise\n",
    "import random\n",
    "from loader import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "from cycler import cycler\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set the color scheme\n",
    "sns.set_theme()\n",
    "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'raw_datasets/'\n",
    "\n",
    "train_dataset = 'DEM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "zYeK3WSMFkAs",
    "outputId": "1a149921-757b-452a-c439-50e0e228074a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:04<00:00, 16.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = data_folder\n",
    "n_sim = 80\n",
    "start_sim = 1\n",
    "dataset_name = 'grid'\n",
    "\n",
    "datasets_folder = 'datasets'\n",
    "if not os.path.exists(datasets_folder):\n",
    "    os.makedirs(datasets_folder)\n",
    "    \n",
    "dataset_dir = datasets_folder + '/train'\n",
    "\n",
    "\n",
    "##################### Use this code to create local pickle file #####################\n",
    "pyg_dataset = create_grid_dataset(dataset_folder, n_sim=n_sim)\n",
    "save_database(pyg_dataset, name=dataset_name, out_path=dataset_dir)\n",
    "\n",
    "train_dataset = load_dataset(dataset_name=dataset_name, dataset_folder=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U8NWW9o1O8kx"
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, scaler_DEM, scaler_WD):\n",
    "    \n",
    "    min_DEM, max_DEM = scaler_DEM.data_min_[0], scaler_DEM.data_max_[0]\n",
    "    min_WD, max_WD = scaler_WD.data_min_[0], scaler_WD.data_max_[0]\n",
    "    normalized_dataset = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        DEM = dataset[idx]['DEM']\n",
    "        WD = dataset[idx]['WD']\n",
    "        norm_DEM = (DEM - min_DEM) / (max_DEM - min_DEM)\n",
    "        norm_WD = (WD - min_WD) / (max_WD - min_WD)\n",
    "\n",
    "        DEM = norm_DEM.reshape(64,64)\n",
    "        WD = norm_WD[:,0].reshape(64,64)\n",
    "        \n",
    "        temp_dict = {}\n",
    "        temp_dict['DEM'] = norm_DEM\n",
    "        temp_dict['WD'] = norm_WD\n",
    "        \n",
    "        normalized_dataset.append(temp_dict)\n",
    "        \n",
    "        \n",
    "        #temp_dict['Input'] = torch.stack((DEM, WD), dim=0)\n",
    "        #WD_transposed = norm_WD[:, 1:].reshape(64,64, -1)\n",
    "        #WD_transposed = WD_transposed.transpose(0, 2)\n",
    "        #WD_transposed = WD_transposed.transpose(1, 2)\n",
    "        #temp_dict['WD'] = WD_transposed\n",
    "    \n",
    "        #normalized_dataset.append(temp_dict)\n",
    "    \n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_DEM = MinMaxScaler() # Can store DEM, VX, VY as one 'input' Scaler\n",
    "scaler_WD = MinMaxScaler()\n",
    "\n",
    "for index in range(len(train_dataset)): # =80\n",
    "    scaler_DEM.partial_fit(train_dataset[index]['DEM'].reshape(-1, 1).cpu())\n",
    "    scaler_WD.partial_fit(train_dataset[index]['WD'].reshape(-1, 1).cpu())\n",
    "    \n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_DEM, scaler_WD)\n",
    "#normalized_test_dataset = normalize_dataset(test_dataset, scaler_DEM, scaler_WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and testing\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "training_dataset, val_dataset = random_split(normalized_train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['DEM', 'WD'])\n",
      "Amount of variables 2\n",
      "Size of WD data (4096, 97)\n",
      "tensor([[0.0025, 0.0891, 0.0892,  ..., 0.1285, 0.1285, 0.1285],\n",
      "        [0.0000, 0.0822, 0.0846,  ..., 0.2054, 0.2054, 0.2054],\n",
      "        [0.0000, 0.0817, 0.1009,  ..., 0.3360, 0.3360, 0.3360],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(normalized_train_dataset[0].keys())\n",
    "\n",
    "# Dataset has two variables, training data DEM, target WD\n",
    "print('Amount of variables', len(normalized_train_dataset[0]))\n",
    "print(f'Size of WD data ({len(normalized_train_dataset[0][\"WD\"])}, {len(normalized_train_dataset[0][\"WD\"][0])})')\n",
    "print(normalized_train_dataset[0]['WD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pgz3cmF4UeQp"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "        layers.append(nn.PReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
    "\n",
    "        self.cnnblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[32, 64, 128], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            outs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return outs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[128, 64, 32], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=2)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "             for block in range(len(channels)-1)]\n",
    "             )\n",
    "\n",
    "    def forward(self, x, x_skips):\n",
    "        for i in range(len(x_skips)):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "\n",
    "        x = self.dec_blocks[-1](x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True,\n",
    "                 bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
    "        encoder_channels = [node_features]+hidden_channels\n",
    "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
    "\n",
    "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x[-1], x[:-1])\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "\n",
    "node_features = 2\n",
    "model = CNN(node_features=node_features, n_downsamples=4, initial_hid_dim=32,\n",
    "            batch_norm=True, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HOBTTiMfIU-N"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "\n",
    "        for timestep in range(normalized_train_dataset[0]['WD'].shape[1]-1):\n",
    "            x = batch['DEM']   # [batch_size, channels, height, width]\n",
    "            x = x.reshape(batch_size, 1, 64, 64)\n",
    "            y0 = batch['WD'][:,:,timestep]\n",
    "            y0 = y0.reshape(batch_size, 1, 64, 64)\n",
    "            x = torch.cat((x,y0), 1)\n",
    "            x = x.to(device)\n",
    "\n",
    "            y = batch['WD'][:,:,timestep+1]\n",
    "            y = y.reshape(batch_size, 1, 64, 64)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "            # Backpropagate and update weights\n",
    "            loss.backward()   # compute the gradients using backpropagation\n",
    "            optimizer.step()  # update the weights with the optimizer\n",
    "            optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluation(model, loader, batch_size, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "\n",
    "            x0 = batch['DEM']   # [batch_size, channels, height, width]\n",
    "            x0 = x0.reshape(batch_size, 1, 64, 64)\n",
    "            x0 = x0.to(device)\n",
    "            y0 = batch['WD'][:,:,0]\n",
    "            y0 = y0.reshape(batch_size, 1, 64, 64)\n",
    "            y0 = y0.to(device)\n",
    "            x = torch.cat((x0,y0), 1)\n",
    "\n",
    "            # Model prediction\n",
    "\n",
    "            timesteps = np.arange(0, normalized_train_dataset[0]['WD'].shape[1], 2)\n",
    "            for timestep in timesteps:\n",
    "            #for timestep in range(normalized_train_dataset[0]['WD'].shape[1]):\n",
    "                y = batch['WD'][:,:,timestep]\n",
    "                y = y.reshape(batch_size, 1, 64, 64)\n",
    "                y = y.to(device)\n",
    "\n",
    "                preds = model(x)\n",
    "\n",
    "                # MSE loss function\n",
    "                loss = nn.MSELoss()(preds, y)\n",
    "\n",
    "                losses.append(loss.cpu().detach())\n",
    "\n",
    "                x = torch.cat((x0,preds), 1)\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XrNWYzHjIgAW"
   },
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16 ### 1, 2, 4, 5, 8, 10, 16, 20, 40, or 80\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#test_loader = DataLoader(normalized_test_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8Old7SUmL0Np"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 \t training loss: 1e-04 \t validation loss: 0.0064\n"
     ]
    }
   ],
   "source": [
    "#create vectors for the training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation(model, val_loader, batch_size=batch_size, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "\n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(\"epoch:\",epoch, \"\\t training loss:\", np.round(train_loss,4),\n",
    "                            \"\\t validation loss:\", np.round(val_loss,4))\n",
    "\n",
    "model = copy.deepcopy(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R0Z0EQMasT3c"
   },
   "outputs": [],
   "source": [
    "#load_path = './models/model1.pth'\n",
    "#model.load_state_dict(torch.load(load_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rN-ARGtbRy1Y"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m evaluation(model, \u001b[43mtest_loader\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_loss)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(model, test_loader, batch_size=5, device=device)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPXLpTAm158g"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-5Qu2_nFlXZ"
   },
   "outputs": [],
   "source": [
    "# select one sample\n",
    "data_id = 60\n",
    "timestep = normalized_train_dataset[0]['WD'].shape[1] - 2   # can only be even numbers for comparing to predictions\n",
    "timestep_pred = int(timestep/2)\n",
    "\n",
    "x = normalized_train_dataset[data_id]['DEM'].reshape(1, 1, 64, 64)\n",
    "WD = normalized_train_dataset[data_id]['WD'][:,timestep+1]\n",
    "\n",
    "x = x.to(device)\n",
    "\n",
    "show_x = x.reshape(64,64).cpu()\n",
    "show_WD = WD.reshape(64,64).cpu()\n",
    "\n",
    "timestep0 = normalized_train_dataset[data_id]['WD'][:,0].reshape(1, 1, 64, 64).to(device)\n",
    "input = torch.cat((x, timestep0), 1)\n",
    "for i in range(int(normalized_train_dataset[0]['WD'].shape[1]/2)):\n",
    "#for i in range(normalized_train_dataset[0]['WD'].shape[1]):\n",
    "    y = model(input)\n",
    "    if i == 0:\n",
    "        pred_WD = y.detach()\n",
    "    else:\n",
    "        pred_WD_new = y.detach()\n",
    "        pred_WD = torch.cat((pred_WD, pred_WD_new), 1)\n",
    "    input = torch.cat((x, y), 1)\n",
    "\n",
    "show_pred_WD = pred_WD[0, timestep_pred, :, :].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD7Fon49FvrV"
   },
   "outputs": [],
   "source": [
    "plt.imshow(show_x, cmap='terrain', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz1IjEhjK-JV"
   },
   "outputs": [],
   "source": [
    "plt.imshow(show_WD, cmap='terrain', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjRQJDMgB0FO"
   },
   "outputs": [],
   "source": [
    "plt.imshow(show_pred_WD, cmap='terrain', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__B2RTw8Nvk-"
   },
   "outputs": [],
   "source": [
    "DEM = scaler_DEM.inverse_transform(x[0].reshape(1,-1).T.cpu())[:,0].reshape(64,64)\n",
    "real_WD = scaler_WD.inverse_transform(WD.reshape(-1,1).cpu()).reshape(1,64,64)[0]\n",
    "pred_WD = scaler_WD.inverse_transform(pred_WD[0, timestep_pred, :, :].reshape(-1,1).cpu()).reshape(64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW7X9Ep0N2hr"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(17,5))\n",
    "\n",
    "diff_WD = real_WD - pred_WD\n",
    "max_WD = max(pred_WD.max(), real_WD.max())\n",
    "max_diff = max(diff_WD.max(), -diff_WD.min())\n",
    "\n",
    "axs[0].imshow(DEM.squeeze(), cmap='terrain', origin='lower')\n",
    "axs[1].imshow(real_WD.squeeze(), vmin = 0, vmax=max_WD, cmap='Blues_r', origin='lower')\n",
    "axs[2].imshow(pred_WD.squeeze(), vmin = 0, vmax=max_WD,cmap='Blues_r', origin='lower')\n",
    "axs[3].imshow(diff_WD.squeeze(), vmin=-max_diff, vmax=max_diff, cmap='RdBu', origin='lower')\n",
    "\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = DEM.min(), vmax=DEM.max()),\n",
    "                            cmap='terrain'), fraction=0.05, shrink=0.9, ax=axs[0])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                            cmap='Blues_r'), fraction=0.05, shrink=0.9, ax=axs[1])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                            cmap='Blues_r'), fraction=0.05, shrink=0.9, ax=axs[2])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=TwoSlopeNorm(vmin=-max_diff, vmax=max_diff, vcenter=0),\n",
    "                            cmap='RdBu'), fraction=0.05, shrink=0.9, ax=axs[3])\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0].set_title('DEM')\n",
    "axs[1].set_title('Real WD (h)')\n",
    "axs[2].set_title('Predicted WD (h)')\n",
    "axs[3].set_title('Difference (h)')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
