{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Useful libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "from urllib.request import urlretrieve\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Additional input\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "!pip install torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "!pip install perlin-noise\n",
        "from perlin_noise import PerlinNoise\n",
        "import random\n",
        "\n",
        "from cycler import cycler\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# Set the color scheme\n",
        "sns.set_theme()\n",
        "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
        "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SsZbVLFE9xe",
        "outputId": "b78a5609-898e-43af-d498-d60c5fa25e28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Collecting perlin-noise\n",
            "  Downloading perlin_noise-1.12-py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: perlin-noise\n",
            "Successfully installed perlin-noise-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def center_grid_graph(dim1, dim2):\n",
        "    '''\n",
        "    Create graph from a rectangular grid of dimensions dim1 x dim2\n",
        "    Returns networkx graph connecting the grid centers and corresponding\n",
        "    node positions\n",
        "    ------\n",
        "    dim1: int\n",
        "        number of grids in the x direction\n",
        "    dim2: int\n",
        "        number of grids in the y direction\n",
        "    '''\n",
        "    G = nx.grid_2d_graph(dim1, dim2, create_using=nx.DiGraph)\n",
        "    # for the position, it is assumed that they are located in the centre of each grid\n",
        "    pos = {i:(x+0.5,y+0.5) for i, (x,y) in enumerate(G.nodes())}\n",
        "\n",
        "    #change keys from (x,y) format to i format\n",
        "    mapping = dict(zip(G, range(0, G.number_of_nodes())))\n",
        "    G = nx.relabel_nodes(G, mapping)\n",
        "\n",
        "    return G, pos\n",
        "\n",
        "def create_grid_dataset(dataset_folder, n_sim, start_sim=1, number_grids=64):\n",
        "    '''\n",
        "    Creates a pytorch geometric dataset with n_sim simulations\n",
        "    returns a regular grid graph dataset\n",
        "    ------\n",
        "    dataset_folder: str, path-like\n",
        "        path to raw dataset location\n",
        "    n_sim: int\n",
        "        number of simulations used in the dataset creation\n",
        "    '''\n",
        "    assert os.path.exists(dataset_folder), \"There is no raw dataset folder\"\n",
        "    grid_dataset = []\n",
        "\n",
        "    graph, pos = center_grid_graph(number_grids,number_grids)\n",
        "\n",
        "    for i in tqdm(range(start_sim,start_sim+n_sim)):\n",
        "\n",
        "        DEM = np.loadtxt(f\"{dataset_folder}/DEM/DEM_{i}.txt\")[:,2]\n",
        "        WD = np.loadtxt(f\"{dataset_folder}/WD/WD_{i}.txt\")\n",
        "#         VX = np.loadtxt(f\"{dataset_folder}\\\\VX\\\\VX_{i}.txt\")\n",
        "#         VY = np.loadtxt(f\"{dataset_folder}\\\\VY\\\\VY_{i}.txt\")\n",
        "\n",
        "        grid_i = convert_to_pyg(graph, pos, DEM, WD)  # VX, VY\n",
        "        grid_dataset.append(grid_i)\n",
        "\n",
        "    return grid_dataset\n",
        "\n",
        "\n",
        "def convert_to_pyg(graph, pos, DEM, WD):  # VX, VY\n",
        "    '''Converts a graph or mesh into a PyTorch Geometric Data type\n",
        "    Then, add position, DEM, and water variables to data object'''\n",
        "    DEM = DEM.reshape(-1)\n",
        "\n",
        "    edge_index = torch.LongTensor(list(graph.edges)).t().contiguous()\n",
        "    row, col = edge_index\n",
        "\n",
        "    data = Data()\n",
        "\n",
        "    delta_DEM = torch.FloatTensor(DEM[col]-DEM[row])\n",
        "    coords = torch.FloatTensor(get_coords(pos))\n",
        "    edge_relative_distance = coords[col] - coords[row]\n",
        "    edge_distance = torch.norm(edge_relative_distance, dim=1)\n",
        "    edge_slope = delta_DEM/edge_distance\n",
        "\n",
        "    data.edge_index = edge_index\n",
        "    data.edge_distance = edge_distance\n",
        "    data.edge_slope = edge_slope\n",
        "    data.edge_relative_distance = edge_relative_distance\n",
        "\n",
        "    data.num_nodes = graph.number_of_nodes()\n",
        "    data.pos = torch.tensor(list(pos.values()))\n",
        "    data.DEM = torch.FloatTensor(DEM)\n",
        "    data.WD = torch.FloatTensor(WD.T)\n",
        "#     data.VX = torch.FloatTensor(VX.T)\n",
        "#     data.VY = torch.FloatTensor(VY.T)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_coords(pos):\n",
        "    '''\n",
        "    Returns array of dimensions (n_nodes, 2) containing x and y coordinates of each node\n",
        "    ------\n",
        "    pos: dict\n",
        "        keys: (x,y) index of every node\n",
        "        values: spatial x and y positions of each node\n",
        "    '''\n",
        "    return np.array([xy for xy in pos.values()])\n",
        "\n",
        "\n",
        "def save_database(dataset, name, out_path='datasets'):\n",
        "    '''\n",
        "    This function saves the geometric database into a pickle file\n",
        "    The name of the file is given by the type of graph and number of simulations\n",
        "    ------\n",
        "    dataset: list\n",
        "        list of geometric datasets for grid and mesh\n",
        "    names: str\n",
        "        name of saved dataset\n",
        "    out_path: str, path-like\n",
        "        output file location\n",
        "    '''\n",
        "    n_sim = len(dataset)\n",
        "    path = f\"{out_path}/{name}.pkl\"\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        os.remove(path)\n",
        "    elif not os.path.exists(out_path):\n",
        "        os.mkdir(out_path)\n",
        "\n",
        "    pickle.dump(dataset, open(path, \"wb\" ))\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "Jcf-x79wFgCr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization to access your Google Drive from Colab.\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# After mounting, you can navigate to a specific folder using the usual UNIX cd command.\n",
        "# Replace 'your_folder_path' with the actual path of your folder inside Google Drive.\n",
        "folder_path = '/content/drive/MyDrive/DSAIE/FLOOD/raw_datasets/'  # Example path\n",
        "\n",
        "%cd \"$folder_path\""
      ],
      "metadata": {
        "id": "rIn361zTHEsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a35f7e-4bef-4b26-daa2-1032bb7bc346"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/DSAIE/FLOOD/raw_datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = folder_path\n",
        "train_dataset = 'DEM/'\n",
        "\n",
        "dataset_folder = data_folder\n",
        "n_sim = 80\n",
        "start_sim = 1\n",
        "dataset_name = 'grid'\n",
        "\n",
        "datasets_folder = 'datasets'\n",
        "if not os.path.exists(datasets_folder):\n",
        "    os.makedirs(datasets_folder)\n",
        "\n",
        "dataset_dir = datasets_folder + '/train'\n",
        "\n",
        "##################### Use this code to create local pickle file #####################\n",
        "pyg_dataset = create_grid_dataset(dataset_folder, n_sim=n_sim)\n",
        "save_database(pyg_dataset, name=dataset_name, out_path=dataset_dir)\n",
        "\n",
        "def load_dataset(dataset_name, dataset_folder='datasets/'):\n",
        "    '''\n",
        "    Loads dataset, composed by a list of pytorch geometric data objects\n",
        "    only accepts files of .pkl format\n",
        "    ------\n",
        "    dataset_name: str\n",
        "        name of the dataset to be loaded\n",
        "    '''\n",
        "\n",
        "    path = f\"{dataset_folder}/{dataset_name}.pkl\"\n",
        "\n",
        "    with open(path, 'rb') as file:\n",
        "        dataset = pickle.load(file)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_dataset = load_dataset(dataset_name=dataset_name, dataset_folder=dataset_dir)"
      ],
      "metadata": {
        "id": "zYeK3WSMFkAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c401c31-d3ec-4f12-c140-be1cf811a82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:14<00:00,  1.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "Pm051i0kLWsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d121d9e-8d55-4075-cad9-a9dcbaa517b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 16128], edge_distance=[16128], edge_slope=[16128], edge_relative_distance=[16128, 2], num_nodes=4096, pos=[4096, 2], DEM=[4096], WD=[4096, 97])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = folder_path\n",
        "test_dataset = 'DEM/'\n",
        "\n",
        "dataset_folder = data_folder\n",
        "n_sim = 20\n",
        "start_sim = 501\n",
        "dataset_name = 'grid_test'\n",
        "\n",
        "datasets_folder = 'datasets'\n",
        "if not os.path.exists(datasets_folder):\n",
        "    os.makedirs(datasets_folder)\n",
        "\n",
        "dataset_dir = datasets_folder + '/test'\n",
        "\n",
        "##################### Use this code to create local pickle file #####################\n",
        "pyg_dataset = create_grid_dataset(dataset_folder, n_sim=n_sim)\n",
        "save_database(pyg_dataset, name=dataset_name, out_path=dataset_dir)\n",
        "\n",
        "test_dataset = load_dataset(dataset_name=dataset_name, dataset_folder=dataset_dir)"
      ],
      "metadata": {
        "id": "il8VXoYzQzw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f74839-ebac-4f17-f5c9-fd775703e723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  8.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_dataset(dataset, scaler_DEM, scaler_WD):\n",
        "    min_DEM, max_DEM = scaler_DEM.data_min_[0], scaler_DEM.data_max_[0]\n",
        "#     min_VX, max_VX = scaler_VX.data_min_[0], scaler_VX.data_max_[0]\n",
        "#     min_VY, max_VY = scaler_VY.data_min_[0], scaler_VY.data_max_[0]\n",
        "    min_WD, max_WD = scaler_WD.data_min_[0], scaler_WD.data_max_[0]\n",
        "    normalized_dataset = []\n",
        "    for idx in range(len(dataset)):\n",
        "        DEM = dataset[idx]['DEM']\n",
        "#         VX = dataset[idx]['VX']\n",
        "#         VY = dataset[idx]['VY']\n",
        "        WD = dataset[idx]['WD']\n",
        "        norm_DEM = (DEM - min_DEM) / (max_DEM - min_DEM)\n",
        "#         norm_VX = (VX - min_VX) / (max_VX - min_VX)\n",
        "#         norm_VY = (VY - min_VY) / (max_VY - min_VY)\n",
        "        norm_WD = (WD - min_WD) / (max_WD - min_WD)\n",
        "        normalized_dataset.append((norm_DEM, norm_WD))\n",
        "    return normalized_dataset\n",
        "\n",
        "\n",
        "# Normalize the inputs and outputs using training dataset\n",
        "scaler_DEM = MinMaxScaler() # Can store DEM, VX, VY as one 'input' Scaler\n",
        "# scaler_VX = MinMaxScaler()\n",
        "# scaler_VY = MinMaxScaler()\n",
        "scaler_WD = MinMaxScaler()\n",
        "\n",
        "for idx in range(len(train_dataset)):\n",
        "    scaler_DEM.partial_fit(train_dataset[idx]['DEM'].reshape(1, -1).T.cpu())\n",
        "#     scaler_VX.partial_fit(train_dataset[idx]['VX'].reshape(train_dataset[0]['VX'].shape[0], -1).T.cpu())\n",
        "#     scaler_VY.partial_fit(train_dataset[idx]['VY'].reshape(train_dataset[0]['VY'].shape[0], -1).T.cpu())\n",
        "    scaler_WD.partial_fit(train_dataset[idx]['WD'].reshape(1, -1).T.cpu())\n",
        "\n",
        "normalized_train_dataset = normalize_dataset(train_dataset, scaler_DEM, scaler_WD)\n",
        "\n",
        "# Split dataset into train, validation, and testing\n",
        "train_percnt = 0.8\n",
        "train_size = int(train_percnt * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "training_dataset, val_dataset = random_split(normalized_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Dataset has two variables, training data DEM, target WD\n",
        "print('Amount of variables', len(normalized_train_dataset[0]))\n",
        "print('Size of DEM data', len(normalized_train_dataset[0][0]))\n",
        "print(f'Size of WD data ({len(normalized_train_dataset[0][1])}, {len(normalized_train_dataset[0][1][0])})')\n",
        "print(normalized_train_dataset[0][0])"
      ],
      "metadata": {
        "id": "U8NWW9o1O8kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d22942-c50e-45d4-d834-0637c3eee86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of variables 2\n",
            "Size of DEM data 4096\n",
            "Size of WD data (4096, 97)\n",
            "tensor([0.4818, 0.4313, 0.3685,  ..., 0.5115, 0.5253, 0.5234])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgz3cmF4UeQp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
        "        layers.append(nn.PReLU())\n",
        "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
        "\n",
        "        self.cnnblock = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnnblock(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channels=[32, 64, 128], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_blocks = nn.ModuleList([\n",
        "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
        "                     batch_norm=batch_norm)\n",
        "            for block in range(len(channels)-1)]\n",
        "            )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            outs.append(x)\n",
        "            x = self.pool(x)\n",
        "        return outs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, channels=[128, 64, 32], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.upconvs = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=2)\n",
        "            for block in range(len(channels)-1)]\n",
        "            )\n",
        "        self.dec_blocks = nn.ModuleList([\n",
        "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
        "                     batch_norm=batch_norm)\n",
        "             for block in range(len(channels)-1)]\n",
        "             )\n",
        "\n",
        "    def forward(self, x, x_skips):\n",
        "        for i in range(len(x_skips)):\n",
        "            x = self.upconvs[i](x)\n",
        "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
        "            x = self.dec_blocks[i](x)\n",
        "\n",
        "        x = self.dec_blocks[-1](x)\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True,\n",
        "                 bias=True):\n",
        "        super(CNN, self).__init__()\n",
        "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
        "        encoder_channels = [node_features]+hidden_channels\n",
        "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
        "\n",
        "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1,\n",
        "                               bias=bias, batch_norm=batch_norm)\n",
        "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1,\n",
        "                               bias=bias, batch_norm=batch_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x[-1], x[:-1])\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "node_features = train_dataset[0]['DEM'].shape[0]\n",
        "model = CNN(node_features=node_features, n_downsamples=4, initial_hid_dim=32,\n",
        "            batch_norm=True, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.train() # specifies that the model is in training mode\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[0]\n",
        "        y = batch[1]\n",
        "\n",
        "        # Model prediction\n",
        "        preds = model(x)\n",
        "\n",
        "        # MSE loss function\n",
        "        loss = nn.MSELoss()(preds, y)\n",
        "\n",
        "        losses.append(loss.cpu().detach())\n",
        "\n",
        "        # Backpropagate and update weights\n",
        "        loss.backward()   # compute the gradients using backpropagation\n",
        "        optimizer.step()  # update the weights with the optimizer\n",
        "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
        "\n",
        "    losses = np.array(losses).mean()\n",
        "\n",
        "    return losses\n",
        "\n",
        "def evaluation(model, loader, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval() # specifies that the model is in evaluation mode\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            x = batch[0]\n",
        "            y = batch[1]\n",
        "\n",
        "            # Model prediction\n",
        "            preds = model(x)\n",
        "\n",
        "            # MSE loss function\n",
        "            loss = nn.MSELoss()(preds, y)\n",
        "            losses.append(loss.cpu().detach())\n",
        "\n",
        "    losses = np.array(losses).mean()\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "HOBTTiMfIU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "# Create the optimizer to train the neural network via back-propagation\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(normalized_test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "XrNWYzHjIgAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create vectors for the training and validation loss\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    # Model training\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
        "\n",
        "    # Model validation\n",
        "    val_loss = evaluation(model, val_loader, device=device)\n",
        "\n",
        "    if epoch == 1:\n",
        "        best_loss = val_loss\n",
        "\n",
        "    if val_loss<=best_loss:\n",
        "        best_model = copy.deepcopy(model)\n",
        "        best_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch%10 == 0:\n",
        "        print(\"epoch:\",epoch, \"\\t training loss:\", np.round(train_loss,4),\n",
        "                            \"\\t validation loss:\", np.round(val_loss,4))\n",
        "\n",
        "model = copy.deepcopy(best_model)"
      ],
      "metadata": {
        "id": "8Old7SUmL0Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rN-ARGtbRy1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}